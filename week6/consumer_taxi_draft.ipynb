{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e47a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b8c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-21 15:17:29</td>\n",
       "      <td>2018-12-21 15:18:57</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:10:16</td>\n",
       "      <td>2019-01-01 00:16:32</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:27:11</td>\n",
       "      <td>2019-01-01 00:31:38</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:46:20</td>\n",
       "      <td>2019-01-01 01:04:54</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2.68</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:19:06</td>\n",
       "      <td>2019-01-01 00:39:43</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>4.53</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0         2  2018-12-21 15:17:29   2018-12-21 15:18:57                  N   \n",
       "1         2  2019-01-01 00:10:16   2019-01-01 00:16:32                  N   \n",
       "2         2  2019-01-01 00:27:11   2019-01-01 00:31:38                  N   \n",
       "3         2  2019-01-01 00:46:20   2019-01-01 01:04:54                  N   \n",
       "4         2  2019-01-01 00:19:06   2019-01-01 00:39:43                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0           1           264           264                5           0.00   \n",
       "1           1            97            49                2           0.86   \n",
       "2           1            49           189                2           0.66   \n",
       "3           1           189            17                2           2.68   \n",
       "4           1            82           258                1           4.53   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0          3.0    0.5      0.5        0.00           0.0        NaN   \n",
       "1          6.0    0.5      0.5        0.00           0.0        NaN   \n",
       "2          4.5    0.5      0.5        0.00           0.0        NaN   \n",
       "3         13.5    0.5      0.5        2.96           0.0        NaN   \n",
       "4         18.0    0.5      0.5        0.00           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    0.3          4.30             2          1   \n",
       "1                    0.3          7.30             2          1   \n",
       "2                    0.3          5.80             1          1   \n",
       "3                    0.3         19.71             1          1   \n",
       "4                    0.3         19.30             2          1   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/green_tripdata_2019-01.csv.gz\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe96541b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument 'newline' not supported in binary mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgreen_tripdata_2019-01.csv.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m csvreader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(csvreader)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\gzip.py:54\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m newline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewline\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n",
      "\u001b[1;31mValueError\u001b[0m: Argument 'newline' not supported in binary mode"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gzip\n",
    "file = gzip.open('green_tripdata_2019-01.csv.gz', \"r\", newline=\"\")\n",
    "csvreader = csv.reader(file, delimiter=',')\n",
    "header = next(csvreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e47a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(\"green_tripdata_2019-01.csv.gz\", \"rt\", newline=\"\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    print(list(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c46f704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m csvreader:\n\u001b[0;32m      2\u001b[0m     key \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvendorId\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;241m0\u001b[39m])}\n\u001b[0;32m      3\u001b[0m     value \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvendorId\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;241m0\u001b[39m])}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "for row in csvreader:\n",
    "    key = {\"vendorId\": int(row[0])}\n",
    "    value = {\"vendorId\": int(row[0])}\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d111cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2018-12-21 15:17:29\n",
      "2 2019-01-01 00:10:16\n",
      "2 2019-01-01 00:27:11\n",
      "2 2019-01-01 00:46:20\n",
      "2 2019-01-01 00:19:06\n"
     ]
    }
   ],
   "source": [
    "test_head = test.head()\n",
    "for index, row in test_head.iterrows():\n",
    "    print(row['VendorID'], row['lpep_pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3811ba89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>2019-01-01 02:51:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>2019-01-01 00:54:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>2019-01-01 00:54:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:19:00</td>\n",
       "      <td>2019-01-01 00:39:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:27:00</td>\n",
       "      <td>2019-01-01 00:37:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00001  2019-01-01 00:30:00  2019-01-01 02:51:55   \n",
       "1               B00001  2019-01-01 00:45:00  2019-01-01 00:54:49   \n",
       "2               B00001  2019-01-01 00:15:00  2019-01-01 00:54:52   \n",
       "3               B00008  2019-01-01 00:19:00  2019-01-01 00:39:00   \n",
       "4               B00008  2019-01-01 00:27:00  2019-01-01 00:37:00   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0           NaN           NaN      NaN                 B00001  \n",
       "1           NaN           NaN      NaN                 B00001  \n",
       "2           NaN           NaN      NaN                 B00001  \n",
       "3           NaN           NaN      NaN                 B00008  \n",
       "4           NaN           NaN      NaN                 B00008  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/fhv_tripdata_2019-01.csv.gz\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1603c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>B00254</td>\n",
       "      <td>2019-01-01 00:33:03</td>\n",
       "      <td>2019-01-01 01:37:24</td>\n",
       "      <td>140.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>B00254</td>\n",
       "      <td>2019-01-01 00:03:00</td>\n",
       "      <td>2019-01-01 00:34:25</td>\n",
       "      <td>141.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>B00254</td>\n",
       "      <td>2019-01-01 00:45:48</td>\n",
       "      <td>2019-01-01 01:26:01</td>\n",
       "      <td>237.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>B00254</td>\n",
       "      <td>2019-01-01 00:37:39</td>\n",
       "      <td>2019-01-01 01:44:59</td>\n",
       "      <td>162.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>B00254</td>\n",
       "      <td>2019-01-01 00:35:06</td>\n",
       "      <td>2019-01-01 01:30:21</td>\n",
       "      <td>237.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23143181</th>\n",
       "      <td>B03040</td>\n",
       "      <td>2019-01-31 23:32:45</td>\n",
       "      <td>2019-01-31 23:32:49</td>\n",
       "      <td>247.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23143183</th>\n",
       "      <td>B03046</td>\n",
       "      <td>2019-01-31 23:37:00</td>\n",
       "      <td>2019-01-31 23:54:00</td>\n",
       "      <td>265.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23143184</th>\n",
       "      <td>B03046</td>\n",
       "      <td>2019-01-31 23:58:00</td>\n",
       "      <td>2019-02-01 00:59:00</td>\n",
       "      <td>265.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23143185</th>\n",
       "      <td>B03046</td>\n",
       "      <td>2019-01-31 23:46:00</td>\n",
       "      <td>2019-02-01 00:21:00</td>\n",
       "      <td>265.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23143211</th>\n",
       "      <td>B03074</td>\n",
       "      <td>2019-01-31 23:45:00</td>\n",
       "      <td>2019-02-01 00:52:00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21323952 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "101                    B00254  2019-01-01 00:33:03  2019-01-01 01:37:24   \n",
       "102                    B00254  2019-01-01 00:03:00  2019-01-01 00:34:25   \n",
       "103                    B00254  2019-01-01 00:45:48  2019-01-01 01:26:01   \n",
       "104                    B00254  2019-01-01 00:37:39  2019-01-01 01:44:59   \n",
       "105                    B00254  2019-01-01 00:35:06  2019-01-01 01:30:21   \n",
       "...                       ...                  ...                  ...   \n",
       "23143181               B03040  2019-01-31 23:32:45  2019-01-31 23:32:49   \n",
       "23143183               B03046  2019-01-31 23:37:00  2019-01-31 23:54:00   \n",
       "23143184               B03046  2019-01-31 23:58:00  2019-02-01 00:59:00   \n",
       "23143185               B03046  2019-01-31 23:46:00  2019-02-01 00:21:00   \n",
       "23143211               B03074  2019-01-31 23:45:00  2019-02-01 00:52:00   \n",
       "\n",
       "          PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "101              140.0          52.0      NaN                 B02356  \n",
       "102              141.0         237.0      NaN                 B00254  \n",
       "103              237.0         236.0      NaN                 B00254  \n",
       "104              162.0          85.0      NaN                 B00254  \n",
       "105              237.0         246.0      NaN                 B00254  \n",
       "...                ...           ...      ...                    ...  \n",
       "23143181         247.0          45.0      NaN                 B03040  \n",
       "23143183         265.0         265.0      NaN                 B03046  \n",
       "23143184         265.0         265.0      NaN                 B03046  \n",
       "23143185         265.0         265.0      NaN                 B03046  \n",
       "23143211         132.0         265.0      NaN                 B03074  \n",
       "\n",
       "[21323952 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test[\"PUlocationID\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795e882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d4df6",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3855574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78caf9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "spark_version = '3.0.3'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3,org.apache.spark:spark-avro_2.12:3.0.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b545500b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fb45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"rides_green,rides_fhv\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fa4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: string, value: string, headers: array<struct<key:string,value:binary>>]\n"
     ]
    }
   ],
   "source": [
    "print(df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e023148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- headers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33521e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_schema = StructType([\n",
    "    StructField(\"vendorId\",StringType()),\n",
    "    StructField(\"pickup_datetime\",StringType()),\n",
    "    StructField(\"dropoff_datetime\",StringType()),\n",
    "    StructField(\"PULocationID\",StringType()),\n",
    "    StructField(\"DOLocationID\",StringType()),\n",
    "    StructField(\"passenger_count\",StringType()),\n",
    "    StructField(\"trip_distance\",StringType()),\n",
    "    StructField(\"payment_type\",StringType()),\n",
    "    StructField(\"total_amount\",StringType()),\n",
    "])\n",
    "fhv_taxi_schema = StructType([\n",
    "    StructField(\"dispatching_base_num\",StringType(),True),\n",
    "    StructField(\"pickup_datetime\",StringType(),True),\n",
    "    StructField(\"dropoff_datetime\",StringType(),True),\n",
    "    StructField(\"PULocationID\",IntegerType(),True),\n",
    "    StructField(\"DOLocationID\",IntegerType(),True),\n",
    "    StructField(\"affiliated_base_number\",StringType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9963aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df = df.selectExpr(\n",
    "        \"CAST(key AS STRING)\", \"CAST(value AS STRING)\"\n",
    ").filter(\n",
    "    col(\"topic\")==\"rides_green\"\n",
    ").select(\n",
    "    col(\"key\"),\n",
    "    from_json(col(\"value\"), green_taxi_schema).alias(\"value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a6e23f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert binary to string key and value\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df1 \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcast(StringType()))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcast(StringType())))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daee8f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "green_df=(df1.withColumn(\"value\", from_json(\"value\", green_taxi_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7e5fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- vendorId: integer (nullable = true)\n",
      " |    |-- pickup_datetime: string (nullable = true)\n",
      " |    |-- dropoff_datetime: string (nullable = true)\n",
      " |    |-- PULocationID: integer (nullable = true)\n",
      " |    |-- DOLocationID: integer (nullable = true)\n",
      " |    |-- passenger_count: integer (nullable = true)\n",
      " |    |-- trip_distance: long (nullable = true)\n",
      " |    |-- payment_type: integer (nullable = true)\n",
      " |    |-- total_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eeb9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"topic\", \"rides_all\") \\\n",
    "  .option(\"checkpointLocation\", \"data/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a9b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "green_taxi_stream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"rides_green\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "  .option(\"value.serializer\",\"org.common.serialization.StringSerializer\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd2d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ride_from_kafka_message(df_raw, schema):\n",
    "    \"\"\" take a Spark Streaming df and parse value col based on <schema>, return streaming df cols in schema \"\"\"\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    # split attributes to nested array in one Column\n",
    "    col = split(df['value'], ', ')\n",
    "\n",
    "    # expand col to multiple top-level columns\n",
    "    for idx, field in enumerate(schema):\n",
    "        df = df.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n",
    "    return df.select([field.name for field in schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa9a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df = parse_ride_from_kafka_message(df_raw=green_taxi_stream, schema=green_taxi_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a453472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.avro.functions import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfa9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFormatSchema = open(\"green_taxi_ride_value.avsc\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f544442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': 'com.datatalksclub.taxi',\n",
       " 'type': 'record',\n",
       " 'name': 'GreenTaxiRide',\n",
       " 'fields': [{'name': 'vendorId', 'type': 'int'},\n",
       "  {'name': 'pickup_datetime', 'type': 'string'},\n",
       "  {'name': 'dropoff_datetime', 'type': 'string'},\n",
       "  {'name': 'PULocationID', 'type': 'int'},\n",
       "  {'name': 'DOLocationID', 'type': 'int'},\n",
       "  {'name': 'passenger_count', 'type': 'int'},\n",
       "  {'name': 'trip_distance', 'type': 'float'},\n",
       "  {'name': 'payment_type', 'type': 'int'},\n",
       "  {'name': 'total_amount', 'type': 'float'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(jsonFormatSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13e57313",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df=green_taxi_stream.select(from_avro(col(\"value\"), jsonFormatSchema).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccdc6eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorId: integer (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ad5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n",
    "    write_query = df.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime=\"5 seconds\") \\\n",
    "        .start()\n",
    "    return write_query # pyspark.sql.streaming.StreamingQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01e8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = sink_console(green_df, output_mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2abf090f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x27d5d2e0400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cce68cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorId: integer (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: long (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- total_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_df=green_taxi_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .withColumn(\"value\", from_json(\"value\", green_taxi_schema)) \\\n",
    "    .select(\"value.*\")\n",
    "green_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47959a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df=green_taxi_stream.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a311b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink_memory(df, query_name, query_template):\n",
    "    write_query = df \\\n",
    "        .writeStream \\\n",
    "        .queryName(query_name) \\\n",
    "        .format('memory') \\\n",
    "        .start()\n",
    "    query_str = query_template.format(table_name=query_name)\n",
    "    query_results = spark.sql(query_str)\n",
    "    return write_query, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66467b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_name = 'green_taxi'\n",
    "query_template = 'select * from {table_name}'\n",
    "write_query, df_PULocationID = sink_memory(df=green_df, query_name=query_name, query_template=query_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa6647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+------------+------------+---------------+-------------+------------+-------------+\n",
      "|vendorId|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|passenger_count|trip_distance|payment_type| total_amount|\n",
      "+--------+---------------+----------------+------------+------------+---------------+-------------+------------+-------------+\n",
      "|       0|               |                |           0|         -13|              1| 2.5639877E-9|         -29|1.0071982E-11|\n",
      "|       0|               |                |           0|         -13|              2| 2.5639877E-9|         -29|1.0071982E-11|\n",
      "|       0|               |                |           0|         -13|              2| 2.5639877E-9|         -29|1.0071982E-11|\n",
      "|       0|               |                |           0|         -13|              2| 2.5639877E-9|         -29|1.0071982E-11|\n",
      "+--------+---------------+----------------+------------+------------+---------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_PULocationID.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3cc30cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f142a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0810cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "|        |green_taxi|       true|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea7bc517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=Row(vendorId=None, pickup_datetime=None, dropoff_datetime=None, PULocationID=None, DOLocationID=None, passenger_count=None, trip_distance=None, payment_type=None, total_amount=None)),\n",
       " Row(value=Row(vendorId=None, pickup_datetime=None, dropoff_datetime=None, PULocationID=None, DOLocationID=None, passenger_count=None, trip_distance=None, payment_type=None, total_amount=None)),\n",
       " Row(value=Row(vendorId=None, pickup_datetime=None, dropoff_datetime=None, PULocationID=None, DOLocationID=None, passenger_count=None, trip_distance=None, payment_type=None, total_amount=None)),\n",
       " Row(value=Row(vendorId=None, pickup_datetime=None, dropoff_datetime=None, PULocationID=None, DOLocationID=None, passenger_count=None, trip_distance=None, payment_type=None, total_amount=None)),\n",
       " Row(value=Row(vendorId=None, pickup_datetime=None, dropoff_datetime=None, PULocationID=None, DOLocationID=None, passenger_count=None, trip_distance=None, payment_type=None, total_amount=None))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"Select * from green_taxi\").tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdaa086e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can't extract value from value#35: need struct type but got string; line 1 pos 61",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSelect value.PULocationID, count(1) from green_taxi group by value.PULocationID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\tools\\spark-3.0.3-bin-hadoop3.2\\python\\pyspark\\sql\\session.py:649\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[0;32m    640\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    :return: :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[1;32mC:\\tools\\spark-3.0.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\tools\\spark-3.0.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Can't extract value from value#35: need struct type but got string; line 1 pos 61"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select value.PULocationID, count(1) from green_taxi group by value.PULocationID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5b1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_ds = green_df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"topic\", \"rides_all\") \\\n",
    "  .option(\"checkpointLocation\", \"data/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a37cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_stream_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"rides_fhv\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4eba139",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_stream_ds = fhv_stream_df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"topic\", \"rides_all\") \\\n",
    "  .option(\"checkpointLocation\", \"data/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63847f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
